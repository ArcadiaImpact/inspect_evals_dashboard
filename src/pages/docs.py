import streamlit as st

# TODO: Refine this page (this has been generated by Claude)
st.title("Documentation")

st.markdown(
    """
    This page provides a **comprehensive guide** to our AI evaluation methodologies, metrics, implementation details, and best practices.

    ## **Contents**

    ### **1. Introduction**
    - [Overview](#overview)
    - [Purpose of the Dashboard](#purpose-of-the-dashboard)
    - [Who Should Use This Dashboard?](#who-should-use-this-dashboard)

    ### **2. Key Concepts**
    - [Glossary](#glossary)
    - [Evaluation Methodologies](#evaluation-methodologies)

    ### **3. Metrics and Scoring**
    - [Performance Metrics](#performance-metrics)
    - [Statistical Analysis](#statistical-analysis)
    - [Score Interpretation](#score-interpretation)

    ### **4. Implementation Details**
    - [Evaluation Pipeline](#evaluation-pipeline)
    - [Data Collection & Preprocessing](#data-collection--preprocessing)
    - [Model Metadata](#model-metadata)

    ### **5. Visualization and Interpretation**
    - [Understanding the Dashboard](#understanding-the-dashboard)
    - [Significance-Based Color Coding](#significance-based-color-coding)
    - [Example Visualizations](#example-visualizations)

    ### **6. Best Practices**
    - [Recommended Evaluation Approaches](#recommended-evaluation-approaches)
    - [Avoiding Common Pitfalls](#avoiding-common-pitfalls)

    ### **7. Frequently Asked Questions (FAQ)**
    - [How do I interpret confidence intervals?](#how-do-i-interpret-confidence-intervals)
    - [What if a model's score is not statistically significant?](#what-if-a-models-score-is-not-statistically-significant)
    - [How are models selected for evaluation?](#how-are-models-selected-for-evaluation)

    ### **8. Data Access & Contribution**
    - [How to Download and Use the Data](#how-to-download-and-use-the-data)
    - [How Did We Calculate the Number of Epochs?](#how-did-we-calculate-the-number-of-epochs)
    - [How to Contribute to the Dashboard](#how-to-contribute-to-the-dashboard)
    - [How to Cite the Dashboard](#how-to-cite-the-dashboard)

    ---

    ## **1. Introduction**
    ### **Overview**
    This dashboard presents AI evaluation results by benchmarking model capabilities using well-defined metrics and statistical methods.

    ### **Purpose of the Dashboard**
    - Enable researchers to compare model performance.
    - Assist developers in refining AI models.
    - Provide transparent insights into evaluation methodologies.

    ### **Who Should Use This Dashboard?**
    - AI researchers analyzing model performance.
    - ML engineers optimizing AI systems.
    - Policymakers & AI governance experts ensuring fairness & safety.

    ---

    ## **2. Key Concepts**
    ### **Glossary**
    - **AI Evaluation**: The process of assessing an AI system’s performance using structured benchmarks.
    - **Model**: A specific AI system such as GPT-4 or Claude 3.5 Sonnet.
    - **Evaluation**: A collection of tasks designed to measure model capabilities.
    - **Capability**: The extent to which an AI system can successfully perform a task.
    - **Propensity**: The likelihood that a model achieves a given evaluation threshold.
    - **Run**: A single instance of a model executing an evaluation.
    - **Task**: An individual question or challenge in an evaluation.
    - **Un-elicited**: An evaluation where the model is not guided toward a specific output.

    ### **Evaluation Methodologies**
    - **Unpaired Analysis**: Compares models without assuming correlation in their responses.
    - **Paired Analysis**: Analyzes model performance on the same inputs, reducing variance and improving statistical significance.

    ---

    ## **3. Metrics and Scoring**
    ### **Performance Metrics**
    - **Score**: A numerical representation of model performance.
    - **F1 Score**: A balance between precision and recall.
    - **Pass@K**: The percentage of runs where a model achieves a passing score.

    ### **Statistical Analysis**
    - **Confidence Intervals**: The range in which the true model performance likely falls.
    - **Score Difference (Δ)**: The difference in scores between two models.

    ### **Score Interpretation**
    - **CI entirely positive** → Model A significantly outperforms Model B.
    - **CI entirely negative** → Model B significantly outperforms Model A.
    - **CI includes zero** → No statistically significant difference.

    ---

    ## **4. Implementation Details**
    ### **Evaluation Pipeline**
    1. **Data Collection** → Gather test cases.
    2. **Scoring** → Assign scores to model outputs.
    3. **Statistical Analysis** → Compute confidence intervals.
    4. **Visualization** → Display results in an interactive format.

    ### **Model Metadata**
    Includes:
    - Model provider, family, training details.
    - Knowledge cutoff date, release date.
    - Context window size, accessibility details.

    ---

    ## **5. Visualization and Interpretation**
    ### **Understanding the Dashboard**
    - Navigate score distributions and confidence intervals.
    - Compare models across different tasks.

    ### **Significance-Based Color Coding**
    - **Green** → Statistically significant improvement.
    - **Red** → Statistically significant decline.
    - **Gray** → No significant difference.

    ### **Example Visualizations**
    - **Bar Charts** → Compare scores across models.
    - **Confidence Interval Plots** → Show statistical significance of score differences.

    ---

    ## **6. Best Practices**
    ### **Recommended Evaluation Approaches**
    - Use **paired analysis** when comparing models on identical inputs.
    - Apply **unpaired analysis** for models tested on different datasets.

    ### **Avoiding Common Pitfalls**
    - **Overinterpreting small differences** → Always consider statistical significance.
    - **Ignoring confidence intervals** → A high score doesn’t mean a model is definitively better.

    ---

    ## **7. Frequently Asked Questions (FAQ)**
    ### **How do I interpret confidence intervals?**
    Confidence intervals show the uncertainty in a model’s score. If a CI excludes zero, the score difference is significant.

    ### **What if a model’s score is not statistically significant?**
    It suggests that the observed difference might be due to random variation rather than a real performance gap.

    ### **How are models selected for evaluation?**
    Models are chosen based on relevance, availability, and importance to ongoing research.

    ---

    ## **8. Data Access & Contribution**
    ### **How to Download and Use the Data**
    - Data is available via API and CSV exports.
    - Includes metadata and scoring details.

    ### **How Did We Calculate the Number of Epochs?**
    - Based on the training cycles used in the model’s learning process.

    ### **How to Contribute to the Dashboard**
    - Submit evaluations through our open-source repository.
    - Propose new benchmark tasks for inclusion.

    ### **How to Cite the Dashboard**
    Use the following format:          
    """
)
